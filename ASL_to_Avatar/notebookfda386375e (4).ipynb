{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12029170,"sourceType":"datasetVersion","datasetId":7568535}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# It's good practice to first uninstall potentially conflicting packages\n!pip uninstall torch torchvision torchaudio transformers accelerate bitsandbytes torchao -y\n\n# Install PyTorch (ensure compatibility with CUDA version on Kaggle GPU)\n!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install OpenMIM\n!pip install -U openmim\n\n# Install MMEngine\n!mim install mmengine\n\n# Install a compatible version of MMCV as per the previous error message\n!pip uninstall mmcv -y # Uninstall current mmcv first to be sure\n!mim install \"mmcv>=2.0.0rc4,<2.2.0\" \n\n# Install specific, potentially older but more stable, versions of transformers and accelerate\n# These versions are chosen to reduce the likelihood of issues with very new torchao features.\n!pip install transformers==4.30.2 \n!pip install accelerate==0.22.0 \n\n# Now install mmdet and mmpose. These should pick up the already installed compatible libraries.\n!mim install \"mmdet>=3.0.0\" \n!mim install \"mmpose>=1.0.0\"\n\n# Install other necessary libraries\n!pip install opencv-python numpy tqdm pandas openpyxl requests","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport mmpose\nimport mmdet\nimport mmcv\nimport mmengine # Ensure this is imported\nimport cv2\nimport numpy as np\nimport os\nimport pandas as pd\nimport requests\nimport subprocess\nimport re\nfrom pathlib import Path\nfrom tqdm import tqdm\n# import tempfile # Not strictly needed now\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Torchvision version: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Current CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n\nprint(f\"MMPose version: {mmpose.__version__}\")\nprint(f\"MMDetection version: {mmdet.__version__}\")\nprint(f\"MMCV version: {mmcv.__version__}\")\nprint(f\"MMEngine version: {mmengine.__version__}\") # <<< ADD THIS LINE\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"Requests version: {requests.__version__}\")\nprint(f\"OpenCV version: {cv2.__version__}\")\n\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(f\"MMCV CUDA version: {get_compiling_cuda_version()}\")\nprint(f\"MMCV compiler version: {get_compiler_version()}\")\n\nffmpeg_check = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)\nif ffmpeg_check.returncode == 0:\n    print(\"ffmpeg found.\")\nelse:\n    print(\"ffmpeg not found. Segmentation will fail.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:11:14.582814Z","iopub.execute_input":"2025-07-01T13:11:14.583101Z","iopub.status.idle":"2025-07-01T13:11:32.574674Z","shell.execute_reply.started":"2025-07-01T13:11:14.583079Z","shell.execute_reply":"2025-07-01T13:11:32.573876Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.1.0+cu121\nTorchvision version: 0.16.0+cu121\nCUDA available: True\nCUDA version: 12.1\nCurrent CUDA device: Tesla P100-PCIE-16GB\nMMPose version: 1.3.2\nMMDetection version: 3.3.0\nMMCV version: 2.1.0\nMMEngine version: 0.10.7\nPandas version: 2.2.3\nRequests version: 2.28.2\nOpenCV version: 4.11.0\n","output_type":"stream"},{"name":"stderr","text":"2025-07-01 13:11:22.052880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751375482.248906     207 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751375482.310114     207 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"MMCV CUDA version: 12.1\nMMCV compiler version: GCC 9.3\nffmpeg found.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport requests\nimport torch # For torch.hub.download_url_to_file\nimport subprocess # For git clone\n\n# --- Directories ---\nBASE_WORKING_DIR = '/kaggle/working/'\nCHECKPOINTS_DIR = os.path.join(BASE_WORKING_DIR, 'checkpoints')\nMMDET_DIR = os.path.join(BASE_WORKING_DIR, 'mmdetection')\nMMPOSE_DIR = os.path.join(BASE_WORKING_DIR, 'mmpose')\n\nos.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n\n# --- Clone MMDetection and MMPose Repositories for Config Files ---\n# We'll clone specific versions/tags if known, otherwise main branch.\n# For mmdet>=3.0.0, let's try to get a recent stable tag or main.\n# For mmpose>=1.0.0, similar approach.\nMMDET_REPO_URL = \"https://github.com/open-mmlab/mmdetection.git\"\nMMPOSE_REPO_URL = \"https://github.com/open-mmlab/mmpose.git\"\nMMDET_TAG = \"v3.1.0\" # A recent stable tag for mmdet 3.x\nMMPOSE_TAG = \"v1.1.0\" # A recent stable tag for mmpose 1.x\n\n\ndef clone_repo_if_not_exists(repo_url, target_dir, tag=None):\n    if not os.path.exists(os.path.join(target_dir, '.git')): # Check if it's a git repo\n        print(f\"Cloning {repo_url} (tag: {tag if tag else 'latest'}) to {target_dir}...\")\n        clone_command = ['git', 'clone']\n        if tag:\n            clone_command.extend(['-b', tag])\n        clone_command.extend([repo_url, target_dir])\n        \n        try:\n            subprocess.run(clone_command, check=True, capture_output=True, text=True)\n            print(f\"Successfully cloned {repo_url} to {target_dir}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Error cloning {repo_url}:\")\n            print(f\"Command: {' '.join(e.cmd)}\")\n            print(f\"Return code: {e.returncode}\")\n            print(f\"Stdout: {e.stdout}\")\n            print(f\"Stderr: {e.stderr}\")\n            raise # Re-raise the exception to stop execution if cloning fails\n    else:\n        print(f\"Repository already exists at {target_dir}, skipping clone.\")\n\nclone_repo_if_not_exists(MMDET_REPO_URL, MMDET_DIR, MMDET_TAG)\nclone_repo_if_not_exists(MMPOSE_REPO_URL, MMPOSE_DIR, MMPOSE_TAG)\n\n\n# --- Pose Estimation Model (RTMPose-L Wholebody) ---\n# Config path now points within the cloned mmpose repo\nlocal_pose_config_file = os.path.join(MMPOSE_DIR, 'configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py')\npose_checkpoint_file_url = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-l_simcc-coco-wholebody_pt-aic-coco_270e-384x288-eaeb96c8_20230125.pth'\nlocal_pose_checkpoint_file = os.path.join(CHECKPOINTS_DIR, 'rtmpose-l_coco-wholebody.pth')\n\n# --- Object Detection Model (Faster R-CNN R50 FPN) ---\n# Config path now points within the cloned mmdetection repo\nlocal_det_config_file = os.path.join(MMDET_DIR, 'configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py')\ndet_checkpoint_file_url = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\nlocal_det_checkpoint_file = os.path.join(CHECKPOINTS_DIR, 'faster_rcnn_r50_fpn_1x_coco.pth')\n\n\ndef download_checkpoint_if_not_exists(url, local_path):\n    if not os.path.exists(local_path):\n        print(f\"Downloading checkpoint {url} to {local_path}...\")\n        try:\n            # Using torch.hub.download_url_to_file for checkpoints\n            torch.hub.download_url_to_file(url, local_path, progress=True)\n            print(\"Download complete.\")\n        except Exception as e:\n            print(f\"Error downloading checkpoint {url}: {e}\")\n            if os.path.exists(local_path): # Clean up partial download\n                os.remove(local_path)\n            raise\n    else:\n        print(f\"Checkpoint {local_path} already exists.\")\n\ndownload_checkpoint_if_not_exists(pose_checkpoint_file_url, local_pose_checkpoint_file)\ndownload_checkpoint_if_not_exists(det_checkpoint_file_url, local_det_checkpoint_file)\n\n# Verify that config files exist at their new paths\nif not os.path.exists(local_pose_config_file):\n    print(f\"ERROR: Pose config file not found: {local_pose_config_file}\")\n    print(\"Please check the mmpose repository structure and path.\")\nelse:\n    print(f\"Pose config file found: {local_pose_config_file}\")\n\nif not os.path.exists(local_det_config_file):\n    print(f\"ERROR: Detection config file not found: {local_det_config_file}\")\n    print(\"Please check the mmdetection repository structure and path.\")\nelse:\n    print(f\"Detection config file found: {local_det_config_file}\")\n    \nprint(\"\\nModel configuration paths updated to use cloned repos. Checkpoint paths set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:11:32.576086Z","iopub.execute_input":"2025-07-01T13:11:32.576805Z","iopub.status.idle":"2025-07-01T13:12:10.512884Z","shell.execute_reply.started":"2025-07-01T13:11:32.576783Z","shell.execute_reply":"2025-07-01T13:12:10.512082Z"}},"outputs":[{"name":"stdout","text":"Cloning https://github.com/open-mmlab/mmdetection.git (tag: v3.1.0) to /kaggle/working/mmdetection...\nSuccessfully cloned https://github.com/open-mmlab/mmdetection.git to /kaggle/working/mmdetection\nCloning https://github.com/open-mmlab/mmpose.git (tag: v1.1.0) to /kaggle/working/mmpose...\nSuccessfully cloned https://github.com/open-mmlab/mmpose.git to /kaggle/working/mmpose\nDownloading checkpoint https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-l_simcc-coco-wholebody_pt-aic-coco_270e-384x288-eaeb96c8_20230125.pth to /kaggle/working/checkpoints/rtmpose-l_coco-wholebody.pth...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 128M/128M [00:07<00:00, 17.3MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Download complete.\nDownloading checkpoint https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth to /kaggle/working/checkpoints/faster_rcnn_r50_fpn_1x_coco.pth...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 160M/160M [00:15<00:00, 10.8MB/s] ","output_type":"stream"},{"name":"stdout","text":"Download complete.\nPose config file found: /kaggle/working/mmpose/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py\nDetection config file found: /kaggle/working/mmdetection/configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py\n\nModel configuration paths updated to use cloned repos. Checkpoint paths set.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from mmdet.apis import init_detector, inference_detector\nfrom mmpose.apis import init_model as init_pose_estimator\nfrom mmengine.registry import DefaultScope # Import DefaultScope class for its static methods\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# --- Manually Define COCO-WholeBody Dataset Info (133 keypoints) ---\n# (COCO_WHOLEBODY_KEYPOINT_NAMES and OFFICIAL_MMPOSE_COCO_WHOLEBODY_SKELETON_LINKS definitions remain here as before)\nCOCO_WHOLEBODY_KEYPOINT_NAMES = [\n    'kpt_0', 'kpt_1', 'kpt_2', 'kpt_3', 'kpt_4', 'kpt_5', 'kpt_6', 'kpt_7', 'kpt_8', 'kpt_9', \n    'kpt_10', 'kpt_11', 'kpt_12', 'kpt_13', 'kpt_14', 'kpt_15', 'kpt_16', 'kpt_17', 'kpt_18', 'kpt_19',\n    'kpt_20', 'kpt_21', 'kpt_22', 'kpt_23', 'kpt_24', 'kpt_25', 'kpt_26', 'kpt_27', 'kpt_28', 'kpt_29',\n    'kpt_30', 'kpt_31', 'kpt_32', 'kpt_33', 'kpt_34', 'kpt_35', 'kpt_36', 'kpt_37', 'kpt_38', 'kpt_39',\n    'kpt_40', 'kpt_41', 'kpt_42', 'kpt_43', 'kpt_44', 'kpt_45', 'kpt_46', 'kpt_47', 'kpt_48', 'kpt_49',\n    'kpt_50', 'kpt_51', 'kpt_52', 'kpt_53', 'kpt_54', 'kpt_55', 'kpt_56', 'kpt_57', 'kpt_58', 'kpt_59',\n    'kpt_60', 'kpt_61', 'kpt_62', 'kpt_63', 'kpt_64', 'kpt_65', 'kpt_66', 'kpt_67', 'kpt_68', 'kpt_69',\n    'kpt_70', 'kpt_71', 'kpt_72', 'kpt_73', 'kpt_74', 'kpt_75', 'kpt_76', 'kpt_77', 'kpt_78', 'kpt_79',\n    'kpt_80', 'kpt_81', 'kpt_82', 'kpt_83', 'kpt_84', 'kpt_85', 'kpt_86', 'kpt_87', 'kpt_88', 'kpt_89',\n    'kpt_90', 'kpt_91', 'kpt_92', 'kpt_93', 'kpt_94', 'kpt_95', 'kpt_96', 'kpt_97', 'kpt_98', 'kpt_99',\n    'kpt_100', 'kpt_101', 'kpt_102', 'kpt_103', 'kpt_104', 'kpt_105', 'kpt_106', 'kpt_107', 'kpt_108', 'kpt_109',\n    'kpt_110', 'kpt_111', 'kpt_112', 'kpt_113', 'kpt_114', 'kpt_115', 'kpt_116', 'kpt_117', 'kpt_118', 'kpt_119',\n    'kpt_120', 'kpt_121', 'kpt_122', 'kpt_123', 'kpt_124', 'kpt_125', 'kpt_126', 'kpt_127', 'kpt_128', 'kpt_129',\n    'kpt_130', 'kpt_131', 'kpt_132'\n]\nOFFICIAL_MMPOSE_COCO_WHOLEBODY_SKELETON_LINKS = [\n    [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12],\n    [5, 6], [5, 7], [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n    [1, 3], [2, 4], [3, 5], [4, 6], [17, 18], [18, 19], [19, 20],\n    [20, 21], [21, 22], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28],\n    [28, 29], [23, 30], [30, 31], [31, 32], [32, 33], [23, 34], [34, 35],\n    [35, 36], [36, 37], [23, 38], [38, 39], [39, 40], [40, 41], [42, 43],\n    [43, 44], [44, 45], [45, 46], [42, 47], [47, 48], [48, 49], [49, 50],\n    [42, 51], [51, 52], [52, 53], [53, 54], [42, 55], [55, 56], [56, 57],\n    [57, 58], [59, 60], [60, 61], [61, 62], [62, 63], [59, 64], [64, 65],\n    [65, 66], [66, 67], [59, 68], [68, 69], [69, 70], [70, 71], [59, 72],\n    [72, 73], [73, 74], [74, 75], [59, 76], [76, 77], [77, 78], [78, 79],\n    [80, 81], [81, 82], [82, 83], [80, 84], [84, 85], [85, 86], [80, 87],\n    [87, 88], [88, 89], [80, 90], [91, 92], [92, 93], [93, 94], [94, 95],\n    [91, 96], [96, 97], [97, 98], [98, 99], [91, 100], [100, 101],\n    [101, 102], [102, 103], [91, 104], [104, 105], [105, 106],\n    [106, 107], [91, 108], [108, 109], [109, 110], [110, 111], [112, 113],\n    [113, 114], [114, 115], [115, 116], [112, 117], [117, 118],\n    [118, 119], [119, 120], [112, 121], [121, 122], [122, 123],\n    [123, 124], [112, 125], [125, 126], [126, 127], [127, 128],\n    [112, 129], [129, 130], [130, 131], [131, 132]\n]\n\n# --- Initialize models ---\n# Initialize detector within the 'mmdet' scope\nprint(\"Initializing detector...\")\nwith DefaultScope.overwrite_default_scope(scope_name='mmdet'): # CORRECTED: Using the classmethod context manager\n    detector = init_detector(local_det_config_file, local_det_checkpoint_file, device=device)\nprint(\"Detector initialized.\")\n\n# Initialize pose estimator within the 'mmpose' scope\nprint(\"Initializing pose estimator...\")\nwith DefaultScope.overwrite_default_scope(scope_name='mmpose'): # CORRECTED: Using the classmethod context manager\n    pose_estimator = init_pose_estimator(local_pose_config_file, local_pose_checkpoint_file, device=device)\n    \n    # --- Manually set the dataset_meta for COCO-WholeBody ---\n    print(\"Explicitly setting COCO-WholeBody dataset_meta...\")\n    \n    new_dataset_meta = {\n        'keypoint_names': COCO_WHOLEBODY_KEYPOINT_NAMES,\n        'num_keypoints': len(COCO_WHOLEBODY_KEYPOINT_NAMES),\n        'skeleton_links': OFFICIAL_MMPOSE_COCO_WHOLEBODY_SKELETON_LINKS,\n    }\n    \n    if hasattr(pose_estimator, 'dataset_meta') and pose_estimator.dataset_meta is not None:\n        pose_estimator.dataset_meta.update(new_dataset_meta)\n        print(\"Updated existing pose_estimator.dataset_meta.\")\n    else:\n        pose_estimator.dataset_meta = new_dataset_meta\n        print(\"Set new pose_estimator.dataset_meta.\")\n            \n    print(f\"Using {len(pose_estimator.dataset_meta['keypoint_names'])} keypoint names.\")\n    print(f\"Using {len(pose_estimator.dataset_meta['skeleton_links'])} skeleton links.\")\n\nprint(\"Pose estimator initialized.\")\nprint(\"Models loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:12:10.513662Z","iopub.execute_input":"2025-07-01T13:12:10.513959Z","iopub.status.idle":"2025-07-01T13:12:16.754465Z","shell.execute_reply.started":"2025-07-01T13:12:10.513934Z","shell.execute_reply":"2025-07-01T13:12:16.753583Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda:0\nInitializing detector...\nLoads checkpoint by local backend from path: /kaggle/working/checkpoints/faster_rcnn_r50_fpn_1x_coco.pth\nDetector initialized.\nInitializing pose estimator...\nLoads checkpoint by local backend from path: /kaggle/working/checkpoints/rtmpose-l_coco-wholebody.pth\nExplicitly setting COCO-WholeBody dataset_meta...\nUpdated existing pose_estimator.dataset_meta.\nUsing 133 keypoint names.\nUsing 128 skeleton links.\nPose estimator initialized.\nModels loaded successfully.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 6 (CORRECTED AGAIN): Pose Extraction Functions\n\nfrom mmpose.apis import inference_topdown\nfrom mmengine.registry import DefaultScope \nimport numpy as np \nimport torch\nfrom tqdm.auto import tqdm\nimport cv2\n\ndef extract_pose_from_frame(frame_bgr, detector_model, pose_estimator_model, person_label_id=0, detection_threshold=0.5):\n    \"\"\"\n    Extracts whole-body pose from a single frame for the largest detected person.\n    This version robustly handles both Tensor and NumPy array outputs from the models.\n    \"\"\"\n    # --- Object Detection ---\n    with DefaultScope.overwrite_default_scope(scope_name='mmdet'):\n        det_results = inference_detector(detector_model, frame_bgr)\n        pred_instances = det_results.pred_instances\n        \n        person_indices = (pred_instances.labels == person_label_id) & (pred_instances.scores > detection_threshold)\n        person_bboxes = pred_instances.bboxes[person_indices]\n\n        if len(person_bboxes) == 0:\n            return None \n\n        areas = (person_bboxes[:, 2] - person_bboxes[:, 0]) * (person_bboxes[:, 3] - person_bboxes[:, 1])\n        \n        # Check if areas is a tensor before calling .cpu()\n        if isinstance(areas, torch.Tensor):\n            largest_person_idx = areas.cpu().numpy().argmax()\n            main_person_bbox = person_bboxes[largest_person_idx:largest_person_idx+1].cpu().numpy()\n        else: # It's already a numpy array\n            largest_person_idx = areas.argmax()\n            main_person_bbox = person_bboxes[largest_person_idx:largest_person_idx+1]\n\n    if main_person_bbox.shape[0] == 0:\n        return None\n\n    # --- Pose Estimation ---\n    with DefaultScope.overwrite_default_scope(scope_name='mmpose'):\n        pose_results = inference_topdown(pose_estimator_model, frame_bgr, main_person_bbox)\n        \n        if not pose_results:\n            return None \n            \n        data_sample = pose_results[0]\n        # These could be Tensors or NumPy arrays\n        keypoints_data = data_sample.pred_instances.keypoints[0]\n        scores_data = data_sample.pred_instances.keypoint_scores[0]\n        \n        # <<< THIS IS THE FIX >>>\n        # Check the type before trying to convert\n        if isinstance(keypoints_data, torch.Tensor):\n            keypoints_np = keypoints_data.cpu().numpy()\n        else: # It's already a numpy array\n            keypoints_np = keypoints_data\n\n        if isinstance(scores_data, torch.Tensor):\n            scores_np = scores_data.cpu().numpy()\n        else: # It's already a numpy array\n            scores_np = scores_data\n        \n        combined_pose_data = np.concatenate((keypoints_np, scores_np[:, np.newaxis]), axis=1)\n    \n    return combined_pose_data\n\ndef process_video_clip_for_pose(video_clip_path, detector_model, pose_estimator_model):\n    \"\"\"\n    Processes a video clip frame by frame to extract poses.\n    Returns a numpy array of (num_frames, num_keypoints, 3).\n    \"\"\"\n    cap = cv2.VideoCapture(str(video_clip_path))\n    if not cap.isOpened():\n        print(f\"Error: Could not open video {video_clip_path}\")\n        return None\n\n    all_frame_poses = []\n    num_keypoints = len(pose_estimator_model.dataset_meta['keypoint_names'])\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    pbar = tqdm(total=total_frames, desc=f\"Extracting Poses from {video_clip_path.name}\", leave=False)\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        pose_data = extract_pose_from_frame(frame, detector_model, pose_estimator_model)\n        \n        if pose_data is not None:\n            all_frame_poses.append(pose_data)\n        else:\n            # Append NaNs if no person is detected or pose fails\n            all_frame_poses.append(np.full((num_keypoints, 3), np.nan))\n        \n        pbar.update(1)\n    \n    cap.release()\n    pbar.close()\n\n    if not all_frame_poses:\n        return None\n        \n    return np.array(all_frame_poses)\n\nprint(\"Corrected pose extraction functions are defined (handles both Tensor and NumPy outputs).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:39:39.989183Z","iopub.execute_input":"2025-07-01T13:39:39.989516Z","iopub.status.idle":"2025-07-01T13:39:40.001006Z","shell.execute_reply.started":"2025-07-01T13:39:39.989492Z","shell.execute_reply":"2025-07-01T13:39:40.000248Z"}},"outputs":[{"name":"stdout","text":"Corrected pose extraction functions are defined (handles both Tensor and NumPy outputs).\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 7 (FINAL-FIX-2): Main Processing Loop - From Video to Pose Data\n\nimport pandas as pd\nimport requests\nimport re\nimport json\nfrom pathlib import Path\n\n# --- Configuration ---\nNUM_SIGNS_TO_PROCESS = 10 # Start with 10 signs, increase later\nCLEANUP_TEMP_FILES = False # Set to True to delete segmented videos after processing\nEXCEL_FILE_PATH = '/kaggle/input/aslvid/asllvd_signs_2024_06_27.xlsx'\nASLLVD_BASE_URL = \"http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/\"\n\n# --- Directories ---\nBASE_WORKING_DIR = Path('/kaggle/working/')\nFULL_VIDEO_DOWNLOAD_DIR = BASE_WORKING_DIR / 'full_source_videos'\nSEGMENTED_VIDEO_DIR = BASE_WORKING_DIR / 'segmented_sign_clips'\nPOSE_DATA_OUTPUT_DIR = BASE_WORKING_DIR / 'pose_data'\nANIMATION_OUTPUT_DIR = BASE_WORKING_DIR / 'animations'\n\n# --- Create all directories ---\nfor d in [FULL_VIDEO_DOWNLOAD_DIR, SEGMENTED_VIDEO_DIR, POSE_DATA_OUTPUT_DIR, ANIMATION_OUTPUT_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# --- Utility Functions (with CORRECTED parser) ---\n\ndef load_excel_data(excel_path, num_samples=None):\n    try:\n        df = pd.read_excel(excel_path)\n        essential_cols = ['full video file', 'start frame of the sign (relative to full videos)', 'end frame of the sign (relative to full videos)', 'Video ID number', 'Class Label']\n        df.dropna(subset=essential_cols, inplace=True)\n        df['start frame of the sign (relative to full videos)'] = df['start frame of the sign (relative to full videos)'].astype(int)\n        df['end frame of the sign (relative to full videos)'] = df['end frame of the sign (relative to full videos)'].astype(int)\n        if num_samples is not None and num_samples < len(df):\n            return df.sample(n=num_samples, random_state=42)\n        return df\n    except Exception as e:\n        print(f\"Error loading Excel file: {e}\")\n        return pd.DataFrame()\n\ndef parse_and_download_video(excel_row, download_dir):\n    \"\"\"\n    Parses various ASLLVD filename formats and downloads the video.\n    This version uses a corrected and robust regular expression.\n    \"\"\"\n    full_video_name = excel_row['full video file'].strip()\n    \n    # This single regex handles all known formats by allowing `_` or `-` before \"scene\"\n    # and correctly escapes the dot in `.mov`.\n    match = re.match(r'^(.*?)[_-]scene(\\d+)-camera(\\d+)\\.mov$', full_video_name)\n    \n    if not match:\n        print(f\"Warning: Could not parse filename: {full_video_name}\")\n        return None\n\n    session_dir, scene_num, cam_num = match.groups()\n    \n    # Construct the URL and local path\n    # The server uses a different filename format for some sessions.\n    # We need to construct the filename as the server expects it.\n    if \"Brady-session\" in session_dir:\n        actual_filename_on_server = f\"scene-{scene_num}-camera{cam_num}.mov\"\n    else:\n        actual_filename_on_server = f\"scene{scene_num}-camera{cam_num}.mov\"\n\n    video_url_path = f\"{session_dir}/{actual_filename_on_server}\"\n    full_url = ASLLVD_BASE_URL + video_url_path\n    \n    # The local filename should be unique\n    local_video_name = f\"{session_dir}__{actual_filename_on_server}\"\n    local_video_path = Path(download_dir) / local_video_name\n\n    if not local_video_path.exists():\n        print(f\"Downloading: {full_url}\")\n        try:\n            response = requests.get(full_url, stream=True, timeout=300)\n            response.raise_for_status()\n            with open(local_video_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192*4):\n                    f.write(chunk)\n            print(f\" -> Saved to {local_video_path.name}\")\n        except requests.exceptions.RequestException as e:\n            print(f\"  -> ERROR downloading {full_url}: {e}\")\n            return None\n            \n    return local_video_path\n\ndef segment_video_ffmpeg(full_path, output_path, start_frame, end_frame):\n    command = ['ffmpeg', '-hide_banner', '-loglevel', 'error', '-i', str(full_path), '-vf', f\"trim=start_frame={start_frame-1}:end_frame={end_frame},setpts=PTS-STARTPTS\", '-an', '-y', str(output_path)]\n    try:\n        subprocess.run(command, check=True)\n        if output_path.exists() and output_path.stat().st_size > 0:\n            return output_path\n        return None\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during ffmpeg segmentation for {full_path.name}: {e}\")\n        return None\n\n# --- Main Loop ---\nexcel_data_df = load_excel_data(EXCEL_FILE_PATH, num_samples=NUM_SIGNS_TO_PROCESS)\ngloss_to_npz_map = {}\n\nfor index, row in tqdm(excel_data_df.iterrows(), total=excel_data_df.shape[0], desc=\"Processing Signs\"):\n    video_id = str(row['Video ID number'])\n    gloss = str(row['Class Label']).strip().upper()\n    start_frame, end_frame = row['start frame of the sign (relative to full videos)'], row['end frame of the sign (relative to full videos)']\n    \n    print(f\"\\nProcessing sign: {gloss} (ID: {video_id})\")\n\n    # 1. Download full video\n    full_video_path = parse_and_download_video(row, FULL_VIDEO_DOWNLOAD_DIR)\n    if not full_video_path:\n        continue\n\n    # 2. Segment the clip\n    safe_gloss = re.sub(r'[^a-zA-Z0-9_]', '', gloss)\n    segmented_clip_name = f\"sign_{video_id}_{safe_gloss}_{start_frame}-{end_frame}.mp4\"\n    segmented_clip_path = SEGMENTED_VIDEO_DIR / segmented_clip_name\n    \n    if not segment_video_ffmpeg(full_video_path, segmented_clip_path, start_frame, end_frame):\n        continue\n\n    # 3. Extract Poses\n    output_npz_path = POSE_DATA_OUTPUT_DIR / f\"poses_{segmented_clip_path.stem}.npz\"\n    if not output_npz_path.exists():\n        pose_data_np = process_video_clip_for_pose(segmented_clip_path, detector, pose_estimator)\n        if pose_data_np is not None:\n            np.savez_compressed(output_npz_path, poses=pose_data_np)\n            print(f\"Saved pose data to {output_npz_path}\")\n        else:\n            print(f\"Pose extraction failed for {segmented_clip_path.name}\")\n            continue\n            \n    # 4. Map Gloss to NPZ file\n    if output_npz_path.exists():\n        relative_path = str(output_npz_path.relative_to(BASE_WORKING_DIR))\n        if gloss not in gloss_to_npz_map:\n            gloss_to_npz_map[gloss] = []\n        if relative_path not in gloss_to_npz_map[gloss]:\n            gloss_to_npz_map[gloss].append(relative_path)\n            \n    if CLEANUP_TEMP_FILES:\n        segmented_clip_path.unlink()\n\n# --- Save the gloss map ---\nGLOSS_MAP_FILE = BASE_WORKING_DIR / 'gloss_to_pose_map.json'\nwith open(GLOSS_MAP_FILE, 'w') as f:\n    json.dump(gloss_to_npz_map, f, indent=4)\n\nprint(f\"\\nAll signs processed. Gloss map saved to {GLOSS_MAP_FILE}\")\nprint(f\"Total unique glosses mapped: {len(gloss_to_npz_map)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:39:42.043873Z","iopub.execute_input":"2025-07-01T13:39:42.044628Z","iopub.status.idle":"2025-07-01T13:42:11.833036Z","shell.execute_reply.started":"2025-07-01T13:39:42.044602Z","shell.execute_reply":"2025-07-01T13:42:11.832162Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Signs:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e94f0663d4c74b7cab23a5c0f73f6f2e"}},"metadata":{}},{"name":"stdout","text":"\nProcessing sign: MOST (ID: 5939)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_5939_MOST_3242-3272.mp4:   0%|          | 0/31 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_5939_MOST_3242-3272.npz\n\nProcessing sign: BLACK+FS-BERRY (ID: 39496)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2008_01_11/scene45-camera1.mov\n -> Saved to ASL_2008_01_11__scene45-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_39496_BLACKFSBERRY_285-345.mp4:   0%|          | 0/61 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_39496_BLACKFSBERRY_285-345.npz\n\nProcessing sign: WATCH (ID: 8777)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2011_07_22_Brady/scene25-camera1.mov\n -> Saved to ASL_2011_07_22_Brady__scene25-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_8777_WATCH_141-180.mp4:   0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_8777_WATCH_141-180.npz\n\nProcessing sign: AFRAID (ID: 857)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2008_01_18/scene39-camera1.mov\n -> Saved to ASL_2008_01_18__scene39-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_857_AFRAID_2130-2155.mp4:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_857_AFRAID_2130-2155.npz\n\nProcessing sign: KIND (ID: 5203)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2008_05_29b/scene14-camera1.mov\n -> Saved to ASL_2008_05_29b__scene14-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_5203_KIND_1180-1221.mp4:   0%|          | 0/42 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_5203_KIND_1180-1221.npz\n\nProcessing sign: RUN (ID: 615)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2008_02_01/scene37-camera1.mov\n -> Saved to ASL_2008_02_01__scene37-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_615_RUN_2780-2845.mp4:   0%|          | 0/66 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_615_RUN_2780-2845.npz\n\nProcessing sign: RED (ID: 7219)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2011_07_19_Brady/scene52-camera1.mov\n -> Saved to ASL_2011_07_19_Brady__scene52-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_7219_RED_1914-1941.mp4:   0%|          | 0/28 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_7219_RED_1914-1941.npz\n\nProcessing sign: INFORM (ID: 4978)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2008_08_13/scene26-camera1.mov\n -> Saved to ASL_2008_08_13__scene26-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_4978_INFORM_4120-4141.mp4:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_4978_INFORM_4120-4141.npz\n\nProcessing sign: BLOSSOM (ID: 1533)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2008_05_12a/scene29-camera1.mov\n -> Saved to ASL_2008_05_12a__scene29-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_1533_BLOSSOM_2240-2275.mp4:   0%|          | 0/36 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_1533_BLOSSOM_2240-2275.npz\n\nProcessing sign: PLAY-AROUND (ID: 6885)\nDownloading: http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/ASL_2011_07_19_Brady/scene36-camera1.mov\n -> Saved to ASL_2011_07_19_Brady__scene36-camera1.mov\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Poses from sign_6885_PLAYAROUND_436-489.mp4:   0%|          | 0/54 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Saved pose data to /kaggle/working/pose_data/poses_sign_6885_PLAYAROUND_436-489.npz\n\nAll signs processed. Gloss map saved to /kaggle/working/gloss_to_pose_map.json\nTotal unique glosses mapped: 10\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Cell 8 (FINAL): Animate Skeletons from Pose Data\n\nimport cv2\nimport numpy as np\nimport json\nfrom pathlib import Path\n\n# --- Configuration ---\n# Directories are inherited from the previous cell\nCONFIDENCE_THRESHOLD = 0.3\nFPS = 30\n\n# --- Load the Gloss Map ---\nGLOSS_MAP_FILE = BASE_WORKING_DIR / 'gloss_to_pose_map.json'\nif not GLOSS_MAP_FILE.exists():\n    print(f\"ERROR: Gloss map not found at {GLOSS_MAP_FILE}. Cannot create animations.\")\nelse:\n    with open(GLOSS_MAP_FILE, 'r') as f:\n        gloss_to_npz_map = json.load(f)\n\n    # --- Use skeleton links from the initialized model ---\n    skeleton_links = pose_estimator.dataset_meta.get('skeleton_links', [])\n    if not skeleton_links:\n        print(\"Warning: Could not get skeleton links from model, animations may be incorrect.\")\n\n    # --- Color Definitions (BGR for OpenCV) ---\n    COLOR_BODY = (255, 128, 0)   # Blue\n    COLOR_FACE = (100, 200, 0)   # Green\n    COLOR_HANDS = (100, 100, 255) # Red\n    COLOR_SKELETON = (200, 200, 200) # Light Grey\n    \n    # Keypoint indices for coloring\n    FACE_KP_INDICES = list(range(23, 91))\n    LEFT_HAND_KP_INDICES = list(range(91, 112))\n    RIGHT_HAND_KP_INDICES = list(range(112, 133))\n\n    print(f\"\\nStarting animation generation for {len(gloss_to_npz_map)} unique glosses...\")\n\n    for gloss, npz_paths in tqdm(gloss_to_npz_map.items(), desc=\"Generating Animations\"):\n        if not npz_paths: continue\n        \n        # Animate the first available video for each gloss\n        npz_relative_path = Path(npz_paths[0])\n        npz_full_path = BASE_WORKING_DIR / npz_relative_path\n        \n        safe_gloss = re.sub(r'[^a-zA-Z0-9_]', '', gloss)\n        animation_output_path = ANIMATION_OUTPUT_DIR / f\"anim_{safe_gloss}_{npz_full_path.stem}.mp4\"\n\n        try:\n            data = np.load(npz_full_path)\n            pose_data = data['poses']\n        except Exception as e:\n            print(f\"Could not load {npz_full_path}: {e}\")\n            continue\n\n        # Filter out low-confidence keypoints to find animation bounds\n        visible_keypoints = pose_data[pose_data[..., 2] > CONFIDENCE_THRESHOLD]\n        if visible_keypoints.shape[0] == 0:\n            print(f\"Skipping '{gloss}': No keypoints found above confidence threshold.\")\n            continue\n\n        # Determine canvas size\n        min_x, min_y = np.min(visible_keypoints[:, :2], axis=0)\n        max_x, max_y = np.max(visible_keypoints[:, :2], axis=0)\n        padding = 50\n        width = int(max_x - min_x) + (2 * padding)\n        height = int(max_y - min_y) + (2 * padding)\n        offset_x = int(min_x - padding)\n        offset_y = int(min_y - padding)\n        \n        # Ensure even dimensions for video codecs\n        width += (width % 2)\n        height += (height % 2)\n\n        # Initialize VideoWriter\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        video_writer = cv2.VideoWriter(str(animation_output_path), fourcc, FPS, (width, height))\n\n        for frame_idx in range(pose_data.shape[0]):\n            keypoints = pose_data[frame_idx]\n            canvas = np.zeros((height, width, 3), dtype=np.uint8) # Black background\n\n            # Draw skeleton\n            for start_idx, end_idx in skeleton_links:\n                if keypoints[start_idx, 2] > CONFIDENCE_THRESHOLD and keypoints[end_idx, 2] > CONFIDENCE_THRESHOLD:\n                    pt1 = (int(keypoints[start_idx, 0] - offset_x), int(keypoints[start_idx, 1] - offset_y))\n                    pt2 = (int(keypoints[end_idx, 0] - offset_x), int(keypoints[end_idx, 1] - offset_y))\n                    cv2.line(canvas, pt1, pt2, COLOR_SKELETON, 1, cv2.LINE_AA)\n            \n            # Draw keypoints\n            for i, (x, y, conf) in enumerate(keypoints):\n                if conf > CONFIDENCE_THRESHOLD:\n                    center = (int(x - offset_x), int(y - offset_y))\n                    color = COLOR_BODY # Default\n                    if i in FACE_KP_INDICES: color = COLOR_FACE\n                    elif i in LEFT_HAND_KP_INDICES or i in RIGHT_HAND_KP_INDICES: color = COLOR_HANDS\n                    cv2.circle(canvas, center, 3, color, -1, cv2.LINE_AA)\n\n            video_writer.write(canvas)\n        \n        video_writer.release()\n\n    print(f\"\\nFinished! All animations saved in: {ANIMATION_OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T13:52:34.977759Z","iopub.execute_input":"2025-07-01T13:52:34.978388Z","iopub.status.idle":"2025-07-01T13:52:36.829010Z","shell.execute_reply.started":"2025-07-01T13:52:34.978363Z","shell.execute_reply":"2025-07-01T13:52:36.828174Z"}},"outputs":[{"name":"stdout","text":"\nStarting animation generation for 10 unique glosses...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Animations:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e42c3bdb32f040469ae020fd7dffa43d"}},"metadata":{}},{"name":"stdout","text":"\nFinished! All animations saved in: /kaggle/working/animations\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Cell 9 (CORRECTED): Normalize Poses and Export to JSON for Three.js\n\nimport numpy as np\nimport json\nimport re\nfrom pathlib import Path\n\n# --- Configuration ---\nJSON_OUTPUT_DIR = BASE_WORKING_DIR / 'json_for_animator'\nJSON_OUTPUT_DIR.mkdir(exist_ok=True)\n\n# Keypoint indices for normalization\nLEFT_HIP_IDX = 11\nRIGHT_HIP_IDX = 12\nLEFT_SHOULDER_IDX = 5\nRIGHT_SHOULDER_IDX = 6\n\n# --- Helper Functions ---\n\ndef normalize_pose_data(pose_data):\n    \"\"\"\n    Normalizes a sequence of poses to be centered and scaled.\n    Input shape: (num_frames, num_keypoints, 3)\n    \"\"\"\n    normalized_data = np.copy(pose_data)\n    \n    for frame_idx in range(normalized_data.shape[0]):\n        frame_keypoints = normalized_data[frame_idx]\n        \n        # 1. Anchor to Hip Center\n        if pose_data[frame_idx, LEFT_HIP_IDX, 2] < 0.3 or pose_data[frame_idx, RIGHT_HIP_IDX, 2] < 0.3:\n            normalized_data[frame_idx, :, :] = np.nan\n            continue\n        hip_center = (frame_keypoints[LEFT_HIP_IDX, :2] + frame_keypoints[RIGHT_HIP_IDX, :2]) / 2.0\n        frame_keypoints[:, :2] -= hip_center\n        \n        # 2. Scale by Shoulder Width\n        if pose_data[frame_idx, LEFT_SHOULDER_IDX, 2] < 0.3 or pose_data[frame_idx, RIGHT_SHOULDER_IDX, 2] < 0.3:\n            normalized_data[frame_idx, :, :] = np.nan\n            continue\n        shoulder_width = np.linalg.norm(frame_keypoints[LEFT_SHOULDER_IDX, :2] - frame_keypoints[RIGHT_SHOULDER_IDX, :2])\n        if shoulder_width < 1e-6:\n            normalized_data[frame_idx, :, :] = np.nan\n            continue\n        frame_keypoints[:, :2] /= shoulder_width\n        \n        normalized_data[frame_idx] = frame_keypoints\n        \n    return normalized_data\n\n\n# --- Main Export Logic ---\n\nGLOSS_MAP_FILE = BASE_WORKING_DIR / 'gloss_to_pose_map.json'\nif not GLOSS_MAP_FILE.exists():\n    print(f\"ERROR: Gloss map not found at {GLOSS_MAP_FILE}. Cannot export.\")\nelse:\n    with open(GLOSS_MAP_FILE, 'r') as f:\n        gloss_to_npz_map = json.load(f)\n\n    # Get metadata from the model and ensure it's in standard Python types\n    keypoint_names = pose_estimator.dataset_meta.get('keypoint_names', [])\n    raw_skeleton_links = pose_estimator.dataset_meta.get('skeleton_links', [])\n    # <<< FIX #1: Convert skeleton links to standard Python ints >>>\n    skeleton_links = [[int(p1), int(p2)] for p1, p2 in raw_skeleton_links]\n    \n    print(f\"\\nStarting JSON export for {len(gloss_to_npz_map)} unique glosses...\")\n\n    for gloss, npz_paths in tqdm(gloss_to_npz_map.items(), desc=\"Exporting to JSON\"):\n        if not npz_paths: continue\n        \n        npz_relative_path = Path(npz_paths[0])\n        npz_full_path = BASE_WORKING_DIR / npz_relative_path\n\n        try:\n            data = np.load(npz_full_path)\n            raw_pose_data = data['poses']\n        except Exception as e:\n            print(f\"Could not load {npz_full_path}: {e}\")\n            continue\n            \n        normalized_poses = normalize_pose_data(raw_pose_data)\n        \n        json_frames = []\n        for frame in normalized_poses:\n            if np.isnan(frame).any():\n                continue\n                \n            keypoints_for_frame = []\n            for x, y, conf in frame:\n                # <<< FIX #2: Convert keypoint data to standard Python floats >>>\n                keypoints_for_frame.append([float(x), float(y), 0.0, float(conf)]) \n            json_frames.append(keypoints_for_frame)\n            \n        if not json_frames:\n            print(f\"Skipping '{gloss}': No valid frames after normalization.\")\n            continue\n\n        output_data = {\n            \"sign_name\": gloss,\n            \"fps\": 60,\n            \"total_frames\": len(json_frames),\n            \"keypoint_names\": keypoint_names,\n            \"skeleton_links\": skeleton_links,\n            \"frames\": json_frames\n        }\n\n        safe_gloss_name = re.sub(r'[^a-zA-Z0-9_]', '', gloss)\n        json_output_path = JSON_OUTPUT_DIR / f\"{safe_gloss_name}.json\"\n        \n        with open(json_output_path, 'w') as f:\n            json.dump(output_data, f)\n\n    print(f\"\\nFinished! All JSON files saved in: {JSON_OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:14:39.318987Z","iopub.execute_input":"2025-07-01T14:14:39.319323Z","iopub.status.idle":"2025-07-01T14:14:39.732396Z","shell.execute_reply.started":"2025-07-01T14:14:39.319300Z","shell.execute_reply":"2025-07-01T14:14:39.731549Z"}},"outputs":[{"name":"stdout","text":"\nStarting JSON export for 10 unique glosses...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Exporting to JSON:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2cb5d90cdc5412ca8cb992fbacaac4f"}},"metadata":{}},{"name":"stdout","text":"\nFinished! All JSON files saved in: /kaggle/working/json_for_animator\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}